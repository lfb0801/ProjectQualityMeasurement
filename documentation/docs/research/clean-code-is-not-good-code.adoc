= Clean Code vs. True Code Quality: Challenging the Conventional Wisdom
:pdf-theme: adoc-style.yml

== Introduction

Over the past decade, the software industry has gravitated toward “Clean Code” principles – a set of guidelines popularized by Robert C. Martin’s *Clean Code* book that emphasize readability, simplicity, and consistency in code. Many teams enforce these ideals through strict *quality gates* (e.g. requiring high unit test coverage, zero linter warnings, adherence to style guides) with the belief that such practices inherently lead to better software. At first glance, it seems obvious: cleaner, more readable code should be easier to maintain and thus higher quality. But how strong is the evidence for this belief? This report examines the assumption that enforcing Clean Code practices and passing automated quality checks necessarily improves code quality. We argue that much of the “research” or justification for Clean Code is either circular or a rehash of long-established principles, and that **code readability alone does not guarantee higher quality**. We further contend that meeting quality gate criteria often leads to superficial fixes without real improvement. Ultimately, the **true measure of software quality emerges from the code’s *behavior and impact over time*** – factors like defect rates, maintenance effort, and team productivity – rather than just the code’s appearance or conformity to rules.

== Clean Code Principles: Old Wisdom Repackaged

Many Clean Code “values” touted today are not novel discoveries but core principles software engineers have known for decades. For example, the advice to use meaningful names, keep functions small and focused, avoid duplicate code, and strive for simplicity can be traced back to classic texts and practices from the 1970s through the 1990s. Martin’s Clean Code book itself acknowledges few empirical sources; instead it offers expert anecdotes and opinions. In fact, a search for academic studies explicitly linking “clean code” to improved outcomes finds little to no rigorous evidence – mostly just references *to* Martin’s book rather than independent validation. The ideas are largely derived from earlier works (such as *Code Complete*, *Refactoring*, and even *Structured Programming* principles) and personal experience, packaged into a modern narrative.

> **Circular reasoning in code quality discussions:** A common pattern is defining good code by qualities like *readability, maintainability,* and *clarity*, then declaring code possessing those qualities as “higher quality” by definition. This becomes a tautology – we assume what we set out to prove. For instance, one might argue *“clean code is better because it’s easier to maintain”* – but if “clean code” simply means code that’s easy to maintain, the statement is self-fulfilling. There is often an implicit *presumption* that readable code is equivalent to good code, rather than concrete evidence of improved outcomes. As an answer on Software Engineering Stack Exchange observed, there isn’t a straightforward case study or dataset showing Clean Code practices alone cause better productivity or fewer bugs; instead, advocates rely on analogies (e.g. comparing messy code to a messy home) and appeals to common sense. In other words, many arguments for Clean Code are intuitive – even valid – but **not empirically proven**. They often restate the goal (easy-to-understand code) as the means to achieve it, resulting in circular logic.

> **“Nothing new under the sun”:** The substance of Clean Code advice is largely timeless programming wisdom. High cohesion and low coupling, clear naming, and avoidance of complexity have been stressed by pioneers like Dijkstra, Parnas, and Knuth. Clean Code’s popularity did refocus attention on code aesthetics and discipline, but it has been criticized for presenting subjective preferences as universal rules and for being rooted in an object-oriented, Java-centric view. In fact, much of the book’s content *“is no longer relevant”* to other paradigms or languages and lacks discussion of data structures or higher-level architecture. This isn’t to say the guidance is *wrong* – only that it repackages established best practices without breaking new ground. The core values (clarity, simplicity, consistency) are sound, but they are principles we’ve known for a long time rather than revolutionary insights from new research.

== Readable Code vs. Software Quality

One of the key tenets of Clean Code is that **improving code readability and understandability will improve code quality**. Indeed, it seems reasonable: if code is easier to read, developers can modify and extend it with fewer mistakes, right? It’s certainly true that when engineers put effort into writing more understandable code, *other people will find it more understandable* – by definition. But this tautology (“readable code is more readable”) doesn’t automatically imply that the code is *better* in terms of reliability, maintainability, or overall quality. We need to disentangle what *aspects* of quality are and are not improved by readability.

Researchers have attempted to quantify code readability and study its effects. For example, in one notable study, Buse and Weimer devised an automated readability metric and found it correlated with certain quality indicators like code change frequency and defect reports. Superficially, this supports the Clean Code notion – code that scored as more readable tended to have fewer defects and changes. However, correlation is not causation. It could be that *difficult* or bug-prone parts of the system naturally accumulate messy, hard-to-read code (i.e. poor design leads to both low readability and more bugs, which is a common-cause scenario). Simply making those files “cleaner” in style may not eliminate the underlying complexity or domain difficulty causing the bugs. In fact, other research casts doubt on how much readability alone captures software quality. A 2018 empirical study of 49 open-source projects found that while code readability (as measured by automated metrics) stayed fairly high and steady, the *design quality* of the software tended to degrade over time as technical debt accumulated – and crucially, **readability did *not* strongly correlate with design quality issues**. The authors reported only a **very weak correlation (τ ≈ 0.15)** between a module’s readability score and the presence of code smells (common design problems). In essence, code could be easy to read line-by-line yet still suffer from deeper structural problems (duplication, poor modularization, etc.) that hurt maintainability. Readability is just one facet of quality, and a superficially clean look doesn’t guarantee sound design.

There are also scenarios where maximizing readability in the small can conflict with other quality attributes. A vivid example comes from performance-critical code. The Clean Code ethos of tiny functions and multiple layers of abstraction can backfire when efficiency is paramount. Game developer Casey Muratori demonstrated this by refactoring a simple computation in a “clean” OOP style versus a straightforward approach. The result? The overly abstracted, clean-version code ran **an order of magnitude slower** – effectively negating years of hardware improvements with unnecessary indirection. While his example is extreme by design, it underscores that “readable” or “clean” code (as defined by certain style rules) isn’t inherently better if it fails other criteria like performance or even clarity at the system level. Sometimes a more complex-looking but direct solution can be *higher quality* for the end user (e.g. faster or more robust) than a perfectly formatted, object-oriented solution that introduces inefficiency or subtle bugs. In short, **readability is beneficial, but it does not equate to overall quality** – and treating it as a proxy can be misleading. Code quality is multi-dimensional, encompassing correctness, robustness, performance, security, and maintainability. Readability contributes to some of those (maintainability, for instance) but not all, and improvements in readability yield diminishing returns if deeper design or behavior issues remain unaddressed.

== The Illusion of Quality Gates and “Passing the Test”

In modern development workflows, it’s common to institute *quality gates*: explicit criteria that code must meet before it’s merged or released. Examples include: **static analysis** (no linter or SonarQube “critical” issues), **unit test coverage** (e.g. at least 80% coverage), **style and formatting checks**, complexity or duplication thresholds, and so on. The intention is to maintain a baseline of code health by automatically preventing obviously low-quality code from slipping through. In practice, however, these gates often encourage a “checklist compliance” mentality. Developers, under pressure to get their code accepted, will do the minimal work required to make the tools happy – which is not the same as actually improving the code in any meaningful way.

&#x20;For example, consider a team that mandates 80% unit test coverage for every module. This is a classic scenario ripe for gaming. It has been observed that teams facing a strict coverage target will start writing superficial or trivial tests *simply to hit the number*, rather than to truly exercise the system. The image above illustrates a real-world case: a “fake” unit test written solely to satisfy a coverage requirement. The test does nothing of substance – in fact, it appears to call a function expecting a `NullPointerException` just to flag that line as “covered” – but it increases the coverage percentage. In one project, such tests were added and entire difficult-to-test classes were **excluded** from coverage calculations to artificially inflate the metric. The result was a reported 99% coverage figure that impressed management, but it was effectively a lie – many parts of the codebase weren’t truly tested, and some tests existed only to placate the coverage tool. This phenomenon is a textbook example of **Goodhart’s Law**, often cited in software: *“When a measure becomes a target, it ceases to be a good measure.”* Once developers are told **“80% coverage or the build fails,”** the metric (coverage %) no longer honestly reflects test quality – it becomes something to hack around. Instead of asking “How can we test this code thoroughly?”, the team thinks “How can we make the coverage tool shut up?”. The underlying goal (code reliability through testing) is undermined by the pursuit of the number.

The same minimal compliance behavior occurs with other quality gates as well. Static analysis warnings get “fixed” by quick tweaks that silence the warning but don’t really improve the code’s clarity or design. For instance, a lint rule might complain about duplicate code, so a developer hastily extracts a helper function to appease the linter – but maybe that new function has no meaningful name or abstraction, and thus adds indirection without benefit. The code is now technically “DRY” (no duplicates) but arguably *harder* to understand than before. Likewise, if a rule says “all TODO comments must be resolved,” developers may simply delete or reword TODOs rather than actually complete the work those comments pointed to. In essence, **teams often treat quality gate outputs as obstacles to be cleared, not hints for genuine improvement**. Passing the automated checks becomes the goal. This is human nature under metrics pressure, and it’s well documented that developer behavior is influenced (sometimes negatively) by metric targets. A manager in one report proudly presented metrics like “0 critical static analysis issues” and “99.8% test coverage” to show the project’s quality. But those pristine numbers only told half the story – they said nothing about whether the code was well-architected or whether the tests were effective. In fact, behind those numbers, developers had found ways to game the system (writing no-op tests, suppressing certain warnings) to make the metrics look good. Thus, a passed quality gate does **not guarantee the code improved in practice**; it might just mean the team found a way to stop the alarm bells from ringing.

None of this is to suggest that static analysis or testing are worthless – on the contrary, they are valuable tools. The point is that **rigidly enforcing their numeric outputs (“quality by numbers”) can create a false sense of security**. Code quality is not a checkbox you tick off; it requires understanding context and intent, which automated rules can’t fully capture. A healthy approach to quality gates is to treat their failures as prompts for thoughtful review, not as absolute must-fix quotas. Unfortunately, when organizations incentivize hitting the targets rather than achieving the underlying quality, the spirit of these practices is lost. As one engineering blog succinctly noted, *“The problem with a metric like code coverage is that it focuses and rewards an outcome, not a behavior”*. If developers are only rewarded for the coverage percentage, they will optimize for that outcome (often by *simply increasing the count of tests* rather than improving test depth). The desired behavior – writing meaningful tests that catch bugs – is not measured, so it gets short-changed. In summary, **passing a quality gate means only that the code meets the letter of some quality rules, not that it’s qualitatively better**. We should be wary of conflating tool satisfaction with true quality.

== Measuring What Matters: Long-Term Outcomes over Aesthetics

If neither visual cleanliness nor metric compliance alone can guarantee quality, how *can* we evaluate code quality reliably? The answer is to observe the code’s *real-world impact* over time – essentially, to measure quality the way one measures the quality of any engineering artifact: by its performance and behavior in practice. For software, this means looking at factors like: **bug rates, production incidents, ease and frequency of changes, longevity of the system, and developer productivity or morale when working on that code**. These are external indicators that code is maintainable and robust (or not). Crucially, they are outcomes that unfold over months and years, not properties you can fully assess in a single code review.

Studies in software engineering back up the idea that **process and history tell us more about code quality than static snapshots do**. Microsoft learned this in dramatic fashion after the buggy Windows Vista release. In an analysis to predict which components were likely to have quality problems, researchers tried many models: some based on the code’s complexity and style, and others based on how the code was developed (e.g. churn and team structure). The winner? **Organizational and historical factors far outranked code metrics in predicting defect-prone modules**. In fact, a module’s bug risk was most accurately forecast by things like *the number of developers who worked on it, how many times it changed, and how organizationally distant the contributors were*, rather than by how “clean” the code looked or how many `if` statements it had. Code churn (the rate of changes to the code over time) also proved to be a strong indicator: components that changed frequently tended to spawn more bugs, likely because constant churn hints at deeper complexity or instability. Prior research has similarly found that *process metrics* (changes, revision history, developer interactions) often predict future bugs better than static *product metrics* like lines of code or cyclomatic complexity. Intuitively, this makes sense – code quality isn’t a fixed attribute; it’s the outcome of a development process. Modules that are touched by many people or undergo frantic modifications may suffer from knowledge fragmentation or quick hacks, leading to lower quality, whereas a well-understood module with a single owner might remain stable and high-quality even if its code isn’t textbook “clean.”

The implication is that **to compare the relative quality of two pieces of software, one should examine their track record over time, not merely their appearance or static analysis scores**. Suppose we have Module A and Module B in a codebase. Module A might violate a bunch of Clean Code rules (perhaps it’s longer, with a few duplicated sections and less-than-ideal naming), whereas Module B is polished to perfection with every function small and every name pristine. Which is higher quality? If Module A in the past year had *zero* production bugs and required minimal developer effort to extend (perhaps its “ugly” parts are at least predictable and well-isolated), while Module B caused multiple outages or absorbed tons of refactoring time due to performance issues, then Module A is clearly the better module in practice. **Quality manifests in how code behaves in the wild and how it affects the people who work on it**. This is why forward-thinking engineering managers focus on metrics like *mean time to restore service (MTTR)*, *change failure rate*, *defect density*, and *developer satisfaction* rather than obsessing over style purity. You could ignore the code’s style entirely and still assess quality by looking at, say, how many defects are found in production per 1000 lines, or how many person-hours are spent maintaining it annually. Those outcome-based measures capture the true cost and reliability of the code. They also can’t be gamed as easily – you can’t fake a reduction in real bugs over time without actually improving the code or process.

None of this is to say that we should never look at code or that code reviews are useless. Rather, it’s an argument against *only* looking at code in isolation. When comparing quality across modules or projects, the **provable** differences come from empirical evidence of how each module performs and ages. Does one module exhibit significantly fewer failures per change? Does another require three times more effort to add features? Those are measurable and meaningful. In contrast, arguing that *“Module X is higher quality because its functions are each under 50 lines and it has no `TODO` comments”* is weak – those are superficial attributes that may have little bearing on long-term outcomes. In fact, a module could satisfy all the Clean Code aesthetics and still be a nightmare to actually use or modify (maybe because the code, while pretty, is over-engineered or the tests give a false sense of security). The ultimate yardstick for code quality is **maintainability** – how easily and safely can we adapt the code to new needs – and that can only be truly measured by observing maintenance in action over time. By focusing on developer behavior and impact (like how code changes propagate, how teams cope with the code, how often issues arise), we ground our quality assessments in reality, not just theory.

== Conclusion

“Clean Code” as a philosophy has done a service in reminding developers to value clarity and discipline in coding. Writing code that is easy to read and understand **does** help reduce certain kinds of problems and makes life easier for the team. However, as we have shown, many of the arguments in favor of Clean Code principles are based on self-evident truths or decades-old wisdom repackaged, rather than novel scientific findings. Readability in code is a virtue – but it is not a panacea for all software ills, nor is it synonymous with quality. Readable code is easier to read, yes, but that doesn’t automatically make it *performant*, *bug-free*, or *well-designed* in a larger sense. We must be careful not to conflate cosmetic or stylistic cleanliness with functional quality. Additionally, enforcing quality gates and metrics can backfire when teams treat the numbers as the goal – leading to gaming the system and only nominal improvements. A green “QA passed” indicator from static analysis or 90% test coverage means little if achieved through minimal-effort tweaks and tests that don’t assert useful behavior.

Ultimately, **the only convincing way to evaluate and compare code quality is to look at how code holds up over time** – how reliably it runs, how costly it is to modify, and how it impacts the people and business depending on it. Two modules might look very different in style, but the one that consistently delivers value with fewer problems is the superior one. By shifting our focus to metrics of developer productivity and defect rates over the code’s lifetime, we gain a fact-based, outcome-oriented view of quality. This is not to diminish the importance of good coding practices, but to put them in perspective. Clean Code ideals should serve as means to an end (better outcomes), not as dogmatic ends in themselves. In practice, a balanced approach works best: encourage readable, maintainable coding practices **and** measure the real effects (are we fixing fewer bugs? adding features faster? getting fewer pager alerts at 2 AM?). Where the two disagree – for example, if a piece of “ugly” code is surprisingly stable and efficient – it’s worth questioning assumptions rather than forcing a rewrite for aesthetics.

In summary, *code quality is ultimately proven in the field, not just in the code review*. Many Clean Code principles are sound engineering heuristics, but they remain heuristics. True quality assurance comes from continuous feedback on the code’s actual behavior and long-term maintainability. Engineering leaders and teams should therefore pay attention to those empirical signals and foster a culture where improving code is about improving outcomes – not just silencing a static analyzer or pleasing a style guide. By doing so, we ensure that our efforts toward cleaner code genuinely translate into higher quality software, rather than merely the appearance of it.

== References

* Martin, Robert C. *Clean Code: A Handbook of Agile Software Craftsmanship*. Prentice Hall, 2008. (Original source of “Clean Code” principles).
* Buse, Raymond P.L., and Westley Weimer. “Learning a Metric for Code Readability.” *IEEE Transactions on Software Engineering*, vol. 36, no. 4, 2010, pp. 546–558.  (Study finding correlation between an automated readability metric and defect frequency).
* Mannan, Umme Ayda, *et al*. “Towards Understanding Code Readability and Its Impact on Design Quality.” *Proc. ACM Workshop on NLP for Software Engineering (NL4SE)*, 2018.  (Study showing weak correlation between readability scores and code smell prevalence).
* Stack Exchange discussion: *“Is there a case study that convincingly demonstrates that clean code improved development?”* – highlights the lack of direct empirical evidence and reliance on anecdotal rationale.
* Thomas, Peter. “Charles Goodhart, Code Coverage and Unintended Consequences.” *Math.random()... blog*, 2012.  (Rant on how coverage targets encourage gaming the metric instead of improving tests).
* “Code Coverage as a Metric.” *Software As Craft* blog, Dec 4, 2024.  (Discusses Goodhart’s Law and how focusing on coverage percentage leads to writing meaningless tests).
* Microsoft Research (Nagappan et al.). *“The Influence of Organizational Structure on Software Quality”* and related studies (circa 2008) – summarized by Lilleaas. Showed that organizational complexity and code churn predict defect-prone modules better than traditional code metrics.
* Lilleaas, August. “The #1 bug predictor is not technical, it’s organizational complexity.” Blog post, Dec 2019.  (Reports Microsoft’s findings that team and process metrics outperformed code metrics for bug prediction in Windows Vista).
* Muratori, Casey. ““Clean” Code, Horrible Performance.” *Computer Enhance* blog, 2023.  (Demonstrates how strictly following certain clean code rules can drastically degrade performance, illustrating trade-offs between readability/abstraction and efficiency).
* Jerry Z. Muller. *The Tyranny of Metrics*. Princeton University Press, 2018. (General treatise on how over-reliance on metrics can be counterproductive – applicable to software quality gates as noted by Software As Craft).
* Various sources on software maintainability and quality measurement (e.g., ISO/IEC 25010 software quality model) for general background on multi-dimensional code quality attributes. (No single metric can capture “code quality” – it spans maintainability, reliability, performance, etc., which should be validated through outcomes).
