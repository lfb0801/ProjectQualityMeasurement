= SonarQube Metrics, Goodhart’s Law, and Long-Term Code Quality
:pdf-theme: adoc-style.yml

== Introduction

Measuring code quality is challenging, especially when developers adapt their behavior to satisfy the metrics. A common scenario is the use of **SonarQube** to enforce standards on new code: the team aims not to decrease existing quality and to incrementally improve the codebase by “boy scouting” (fixing issues as you encounter them). The dilemma is that developers often do the bare minimum to make SonarQube “stop complaining,” rather than implementing deeper, long-term fixes. This raises questions about the long-term effectiveness of such metrics-based approaches. In particular, **Goodhart’s Law** looms large: *“When a measure becomes a target, it ceases to be a good measure.”* Once hitting SonarQube’s targets (zero issues, certain coverage percentage, etc.) becomes the goal, the metric may no longer reflect true code quality. This report examines evidence for how code quality metrics can be misused (supporting the hypothesis that SonarQube can encourage superficial fixes), the limitations of the “boy scout rule” in ensuring continuous improvement, and the difference between measuring **code quality** versus **maintainability**. The findings strongly support the hypothesis that focusing on SonarQube’s metrics alone can undermine long-term quality improvements.

== The Metrics Trap and Goodhart’s Law

Metrics like those in SonarQube are meant as proxies for code quality (e.g. counts of code smells, test coverage %, complexity scores). Goodhart’s Law suggests a danger: once developers start chasing the metric itself, the metric loses its validity as an indicator of quality. In practice, this means teams risk writing code *just* to satisfy SonarQube’s rules or thresholds, instead of truly improving the software design or readability. There is documented evidence of this phenomenon. For example, one article describes how enforcing a test coverage percentage can lead developers to game the system – by excluding untested code from analysis or even writing dummy tests and code that serve no purpose except to boost the coverage number. In other words, the team “develops their code in exact accordance with the metrics” rather than the spirit of quality.

In the context of SonarQube, similar gaming can occur. Developers may fix or silence only the warnings SonarQube flags, without addressing the underlying issue the rule is meant to highlight. One developer bluntly noted: *“We in fact use SonarQube, and developers quick fix SonarQube issues with only the intention to remove that error and with no thought to improve the readability. Unfortunately this is not helping us make the code more readable.”*. This real-world observation exemplifies Goodhart’s Law: the **metric (zero linter warnings)** became the goal, and developers achieved it in the easiest way – by superficial changes – rather than actually enhancing code clarity (the original quality intent behind the metric).

Over time, the overemphasis on metric compliance can create a false sense of code quality. The codebase may appease SonarQube (no code smells, all tests “passing”), yet still suffer from deeper design problems or poor readability that the tool’s quantitative rules cannot detect. Research on static analysis tools supports this concern. A long-term view shows that developers tend to **ignore many automated warnings** outright or only fix a small subset of them. A survey and data analysis by Mosulet (2021) found that on average only about **10.7% of static analysis issues** (from SonarQube and similar tools) ever got fixed by developers. Prior studies over two decades have noted a consistently low fix-rate for such tool alerts, indicating that developers either skip or only superficially address most issues the tools report. In many cases, teams might mark issues as “Won’t Fix” or simply live with the warning once the pressure to immediately clean up the metric passes. This low engagement with issues suggests that using the metric as an enforcement tool (e.g. quality gates that block builds) might get developers to eliminate warnings in the short term, but it doesn’t guarantee the code is actually better – it may just be *silencing* the symptom.

Not all SonarQube detections are equally important, either. Some empirical studies have examined whether fixing SonarQube rule violations actually leads to measurable improvements in software quality (like fewer bugs or less change-prone code). The results can be sobering: one large study of 33 Apache projects found that classes with SonarQube-detected issues were **no less likely to have bugs or to churn** than classes without such issues. In fact, for certain types of issues (minor “code smells”), there was *no difference in fault-proneness* between code that Sonar flagged and code it didn’t. The overall differences in change frequency were very small. This implies that many SonarQube rule violations (especially minor maintainability issues) are not strongly correlated with real defects or instability. In practice, a code file can be perfectly clean by SonarQube standards and still harbor design flaws or logic bugs that the tool cannot detect; conversely, a file might have several code smell warnings (e.g. slightly long methods or naming issues) yet function robustly with no bugs. **Investing developer effort to appease every metric might yield diminishing returns**, or distract from more impactful improvements. This is another facet of Goodhart’s Law: optimizing for the measured aspects of quality (like “number of code smells”) can misdirect effort away from unmeasured aspects that matter (such as usability of the code, correct behavior, etc.).

== Superficial “Boy Scout” Fixes vs. Real Improvements

The “**boy scout rule**” – *“leave the code better than you found it”* – is often proposed as a strategy to gradually improve code quality. In theory, if every commit cleans up a few problems (refactors a bit of messy code or fixes a SonarQube warning in the vicinity), the overall system quality will continuously increase. This principle has merit and is embraced in many teams as part of the culture of cleanliness and continuous refactoring. It can help prevent **“broken windows” syndrome**, where letting small messes accumulate leads to more decay over time. By encouraging developers to take ownership of code quality and fix issues whenever they touch code, teams hope to pay down technical debt incrementally even without dedicated refactoring sprints.

However, in practice the boy scout approach has notable **limitations**. One limitation is that it often leads to **local, superficial fixes** (e.g. renaming a variable, cleaning up a small section of code) rather than addressing architectural or systemic problems. As one engineer pointed out, *“Boyscouting should be local and limited… It will not work for large and time-consuming refactorings, but will work just fine for smaller things such as code smells.”*. In other words, you can use the boy scout rule to fix minor issues on the fly, but **larger design flaws or technical debt items usually require dedicated effort** outside the scope of a quick drive-by fix. If a codebase has deep architectural problems – say, a “God class” or a tangled module dependency – those won’t be solved by a few boy-scouted tweaks; they need conscious re-engineering which the boy scout philosophy might postpone indefinitely. Teams can fall into a trap of polishing the surface (resolving linter warnings, reformatting code) while neglecting more fundamental redesigns.

Another **deficiency of the boy scout method** is the risk of diminishing accountability and learning. If developers are constantly cleaning up each other’s code in passing, the original author of a problematic section might never learn of the issue or take responsibility for improving. A senior developer argued that in software, unlike in camping, the “mess” has an identifiable owner (thanks to version control) and that owner should be involved in the fix. Otherwise, *“don’t take away the chance for an individual to learn, and this is exactly what happens when you fix it yourself. Great chance the person who made a mistake will not know about the correction… potentially \[to] repeat the mistake.”*. This perspective highlights that the boy scout rule, if applied indiscriminately, could allow the same mistakes to recur because the quick cleaner-uppers treat the symptom but not the root cause (the developer’s understanding or the team’s process). Instead, the suggestion is to **loop in the “offender”** – for example, via code reviews or by creating a ticket and asking the original author to improve their code – so that they learn and the team addresses why the issue happened in the first place.

Additionally, there is a **prioritization problem**: not all code problems are worth fixing just because you see them. In a business context, time is limited and developers are often under pressure to deliver features. Stopping to clean every little thing (especially if SonarQube flags dozens of minor issues) can conflict with feature delivery. Some fixes have “no immediate value for the business,” as critics note, so management might not support spending time on them. The boy scout rule doesn’t provide guidance on which battles to pick – without discipline, a team could waste effort micro-optimizing trivial matters (e.g. tiny stylistic issues) while bigger design debt goes untouched. Indeed, some agile teams explicitly set aside separate refactoring tasks for larger improvements, since boy scouting alone won’t suffice for, say, breaking up a monolith or rewriting a brittle module.

In summary, the boy scout method on its own can lead to **fragmented, shallow improvements**. Developers might fix the easy warnings (to please SonarQube or to make the diff look cleaner) but avoid the more complex cleanup that isn’t immediately required. Over reliance on boy scouting can also diffuse responsibility – everyone fixes little things, but no one tackles the big things. This doesn’t mean the principle is without value (it does foster a culture of care and incremental cleanup), but it shows why boy scouting combined with naive metric goals might not guarantee long-term quality. Without a strategic view, you risk lots of activity (many tiny fixes) with little positive impact on the overall maintainability or architecture.

== Quality vs. Maintainability: What Do We Measure?

It’s important to distinguish **“code quality”** in general from the more specific attribute of **“maintainability.”** Code quality is an umbrella term that can include correctness, reliability, performance, security, style, readability, and more. Many of these qualities are hard to quantify directly. *Maintainability*, on the other hand, usually refers to how easily the code can be understood, modified, and extended by developers – essentially, the effort required to make changes in the future. Maintainability is widely regarded as one of the more *measurable* aspects of code quality because it correlates with certain structural properties of the code (complexity, coupling, duplication, size, etc.) that static analysis tools like SonarQube can analyze.

SonarQube’s design reflects this focus: it evaluates code against a set of rules and calculates metrics across **seven axes of quality** (including architecture/design, duplications, unit testing, complexity, coding practices, comments, and potential bugs). Many of these are directly tied to maintainability. For example, SonarQube computes a **“Maintainability rating”** which is derived from the estimated technical debt – i.e. the effort (in man-days or hours) to fix all the code smells and maintainability issues it has detected. This gives a quantifiable figure (often expressed as a debt ratio or a letter grade A–E) indicating how maintainable the codebase is in terms of known issues. Such metrics make maintainability *tangible*; one can track the technical debt over time or set a threshold (e.g., fail the build if the new code introduces more than a certain amount of debt). Size metrics like **lines of code (LOC)**, file complexity scores, or function cognitive complexity also feed into maintainability assessments and are objectively measurable.

However, **measuring maintainability is not the same as measuring overall code quality.** A maintainable codebase is one that *should* be easier to work with, but it might still contain functional bugs or lack needed features – quality is multi-dimensional. Moreover, even within maintainability, the metrics we have are imperfect proxies for what truly makes code easy to modify. Recent empirical research found that many qualities developers associate with “good code” – notably **readability, understandability, and good structure** – *“lack clear definitions and are difficult to capture”* with automated metrics. Developers in the study emphasized things like clear naming, simplicity, and logical organization as key to quality, but those are not directly measurable by SonarQube’s rule-checkers in any comprehensive way. You might get a complexity score or a style rule violation, but such metrics don’t fully capture whether a piece of code is *easy to comprehend*. In fact, another finding was that common code metrics often **“do not capture quality improvements as perceived by software developers.”** Simply improving a metric (say, reducing cyclomatic complexity from 15 to 10 in a function) doesn’t always align with what human developers would consider a meaningful improvement in clarity or design.

One reason is that **context matters**. A block of code might be complex (high complexity score) but very well documented and perfectly acceptable given its task, thus still understandable; conversely, a code snippet might technically satisfy all the style rules and have low complexity, but be written in an obscure way that confuses readers. **Maintainability metrics tend to focus on what’s easily quantifiable** – e.g. length of methods, nesting depth, number of code smells – which definitely relate to maintainability but don’t tell the whole story. Code quality in the holistic sense includes qualities like *correctness, robustness, and alignment with requirements*, which are not covered by static analysis metrics at all.

That said, metrics for maintainability are still **valuable indicators** when used properly. Unlike subjective attributes such as “code elegance,” maintainability metrics give teams concrete feedback. For instance, SonarQube’s **Technical Debt Ratio** (the ratio of remediation cost to development cost) provides a high-level gauge of how much muck is in the code relative to its size. If this ratio is growing over time, it’s a red flag that maintainability is deteriorating (e.g. developers are introducing more issues than they’re fixing). Teams can watch trends: an increase in average module complexity or duplicate code can predict harder maintenance down the line. **Size metrics** like LOC or number of files, when correlated with issue counts, can identify hot spots that might need refactoring (for example, a large file with many code smells is likely a pain point).

Crucially, to avoid the Goodhart’s Law pitfall, teams should use these metrics as *diagnostic tools* rather than absolute goals. It’s more effective to combine SonarQube’s static metrics with **historical/behavioral data** about the code. For example, if a certain component has rising technical debt *and* has caused multiple production bugs or many change requests, that’s a strong signal to prioritize a real fix. Conversely, if SonarQube shows a few minor issues in a module that rarely changes and hasn’t caused problems, it may be fine to leave those alone for now. Research supports this nuanced approach: one study proposed correlating static analysis warnings with higher-level design problems (like architectural “smells”). The authors found that about **one-third of SonarQube warnings were “false alarms”** in the sense that they did not correlate with any architectural problems – these could be safely ignored – whereas a subset of warnings did co-occur with serious design issues and should be prioritized. By focusing on the static issues that matter most (for maintainability or architecture) and not obsessing over every minor metric, a team can better align the metric with the true goal of quality.

In practice, some SonarQube checks do target deeper maintainability concerns. For example, SonarQube can detect cycles in package dependencies (architecture), overly large classes or complex functions (design maintainability), and even provide an **“architectural constraint”** evaluation in its higher-level quality gate. These architectural metrics, combined with the code change history, can highlight areas of the system that need redesign — something a raw code smell count alone wouldn’t reveal. A metric like **Lines of Code** per module might not mean much by itself, but if a file grows to thousands of LOC over time, that trend coupled with high complexity could indicate an **erosion of modularity**. So, the value of Sonar’s measurements comes out when they are *contextualized*: used in comparisons over time or to guide where engineers should perform deeper analysis.

== Conclusion

The research and evidence strongly support the hypothesis that relying on SonarQube’s code quality metrics as a direct yardstick of quality can lead to counterproductive behaviors and only superficial improvements. Goodhart’s Law is clearly at play: once teams fixate on “what SonarQube wants” (e.g., 0 code smells, 80% test coverage), developers naturally find the path of least resistance to hit those targets – whether by writing shallow tests, tweaking code just to silence warnings, or ignoring the warnings altogether – thereby undermining the very purpose of the metrics. In the long run, this metric-driven compliance can create a codebase that looks clean to the tool but still harbors poor quality in areas the tool doesn’t measure (like logic clarity or appropriate architecture). It also risks complacency: teams might pat themselves on the back for an “A” SonarQube rating while the actual developer experience of changing the code remains painful.

The **“boy scout” strategy alone is not sufficient** to guarantee long-term improvement either. While it encourages continual tidying, it tends to address symptoms (small code smells) rather than root causes, and can even impede learning if misapplied. Long-term code quality requires tackling larger refactors and architectural fixes that a purely opportunistic, metric-driven approach may never get around to. In essence, you cannot *metric* your way to clean architecture; conscious design effort and team knowledge are needed, beyond what SonarQube can dictate.

However, this doesn’t mean SonarQube and similar tools have no value – in fact, they are very useful for maintaining a baseline level of code health and catching regressions. The key is **using metrics wisely**. When SonarQube flags an issue, developers should treat it as a prompt to think deeper about the code rather than an item to quickly appease. Teams should also refine their use of the tool: for example, customize the rule set to focus on issues known to correlate with real problems, and use the **quality gate** concept not as a hard target to game but as a safety net (e.g., “don’t add more debt than you pay off”). By coupling SonarQube metrics with qualitative review and historical insights (e.g. which files cause outages or slow down development), organizations can ensure the metric remains a *meaningful indicator* rather than just a number to hit.

In conclusion, the hypothesis is confirmed – **when code quality metrics become goals, developers often meet the goals in ways that don’t truly improve the code**. Goodhart’s Law manifests in software engineering as shallow fixes that quiet the tool but fail to enhance maintainability or readability in a significant way. The boy scout rule, while positive in spirit, does not fully counteract this tendency and has its own pitfalls if not complemented by broader technical leadership and planning. True long-term quality improvement comes from aligning metric-driven practices with the underlying qualitative goals: focusing on maintainability improvements that matter, educating developers on *why* an issue is flagged, and keeping metrics in their place as guides rather than absolute objectives. SonarQube can be a powerful ally for quality – but only if we remember that the real goal is high-quality, maintainable code, not just a high quality score.

== Sources:

1. Hamer, S. & Quesada-López, C. (2023). *Students’ perceptions of integrating a contribution measurement tool in software engineering projects.* – Reveals that metric-driven tools (Git/SonarQube) can lead to “box-ticking” behavior by students, focusing on pleasing the tool over genuine improvements.
2. Tóth, Z. et al. (2015). *Comparison of Static Analysis Tools for Quality Measurement of RPG Programs.* – Notes weaknesses in static analysis metrics and how developers often meet the letter of the rules without improving architecture, echoing Goodhart’s Law.
3. Ballario, M. (2022). *Research, Implementation and Analysis of Source Code Metrics in Rust-Code-Analysis.* – Discusses developers’ perception of metrics and tendency to fix warnings as “bugs” regardless of context, leading to minimal compliance instead of thoughtful refactoring.
4. Mosulet, P.P. (2021). *An Analysis of the Usage and Impact of Static Code Analysis Tools.* – Found that only \~10% of SonarQube-reported issues were fixed, and most warnings lingered or were ignored, indicating metric fatigue and selective attention. Also reported developers view such tools as mechanical gatekeepers rather than learning tools.
5. Lenarduzzi, V. et al. (2020). *“Some SonarQube issues have a significant but small effect on faults and changes” (Journal of Systems and Software).* – A large-scale empirical study showing that many SonarQube rule violations did **not** significantly correlate with fault-proneness or high change frequency, reinforcing that blindly fixing those issues might not yield tangible benefits. Highlights need to prioritize which issues to fix.
6. Bouhier, C. (2017). *“Why the boy scout rule is a very bad idea in software” (LinkedIn article).* – Argues that the boy scout rule can be counterproductive in a team setting: the original author of code should fix their mistakes to learn, rather than others constantly cleaning up after them. Suggests using code reviews and mentorship (“situational leadership”) instead of ad-hoc cleanups for better long-term outcomes.
7. Anton9 (2023). *“Boy Scout Rule and Its Limitations” (Dev.to article).* – Emphasizes that boy scouting is useful only for small-scale fixes and hygiene, and warns that **large refactorings require dedicated effort** outside of the boy scout approach. Advises teams to limit the scope of boy scout changes to avoid PRs that mix refactoring with functional changes.
8. Börstler, J. et al. (2023). *“Developers talking about code quality” (Empirical Software Engineering).* – A study via interviews showing that developers consider **readability, structure, and comprehensibility** as prime factors in code quality, yet these are hard to measure objectively. Also finds that traditional code metrics often don’t align with developers’ notion of improved quality, underlining the gap between maintainability metrics and true code quality.
9. Dzone Editorial (2021). *“Project Hygiene, Part 2: Combatting Goodhart’s Law…”* – Discusses Goodhart’s Law in software and provides concrete examples of teams gaming metrics like code coverage at the expense of real quality. Reinforces the lesson that metrics should guide inquiry, not become the *raison d’être*.
10. Robredo, M. et al. (2024). *“On the correlation between Architectural Smells and Static Analysis Warnings” (preprint).* – Suggests a method to improve the impact of static analysis by focusing on warnings that coincide with architectural design problems. Found that one-third of static warnings were not associated with any deeper issue (hence low priority), and that prioritizing warnings by their likelihood of linking to architecture problems can make remediation more effective. This points the way toward combining SonarQube metrics with architectural insight to achieve meaningful, long-term quality gains.
